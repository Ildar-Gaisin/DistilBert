{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8dce700-426a-4deb-a966-257cfc082d40",
   "metadata": {},
   "source": [
    "# This notebook will pretrain a BERT model on the WikiText-2 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42fec08-4fe2-4fd4-a212-a58a495c147a",
   "metadata": {},
   "source": [
    "## Let's first load the WikiText-2 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e12e0f1-42ea-4558-b2da-a9687dd15194",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's get the data..let's try to use the Hugging Face API\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the wikitext-2-raw-v1 configuration of the wikitext dataset\n",
    "raw_datasets = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "\n",
    "# The 'raw_datasets' object will now contain the dataset, typically split into\n",
    "# 'train', 'validation', and 'test' splits.\n",
    "# You can access them like this:\n",
    "train_data = raw_datasets[\"train\"]\n",
    "validation_data = raw_datasets[\"validation\"]\n",
    "test_data = raw_datasets[\"test\"]\n",
    "\n",
    "print(f\"Number of training examples: {len(train_data)}\")\n",
    "print(f\"Number of validation examples: {len(validation_data)}\")\n",
    "print(f\"Number of test examples: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a7bd45-2eec-4ce4-b1e7-eed125f55633",
   "metadata": {},
   "source": [
    "## Let's not create a custom Dataset and Collate function for dynamic padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "id": "8edb960f-504a-4a86-ad59-15b8e9c95216",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from transformers import AutoTokenizer, DataCollatorForLanguageModeling\n",
    "import torch\n",
    "\n",
    "checkpoint = \"google-bert/bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, tokenizer, data, max_length):\n",
    "        self.tokenizer=tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        L = [p[\"text\"] for p in data]\n",
    "        self.NSP_data = []\n",
    "\n",
    "        for paragraph in L:\n",
    "            cand_sentences = paragraph.split(' . ')\n",
    "            number_of_sentences = len(cand_sentences)\n",
    "            if number_of_sentences<2:\n",
    "                continue\n",
    "            else:\n",
    "                for i in range(number_of_sentences-1):\n",
    "                    if random.random()<0.5:\n",
    "                        self.NSP_data.append(((cand_sentences[i], cand_sentences[i+1]), 1))\n",
    "                else:\n",
    "                    repla_sent = random.choice(L)\n",
    "                    repla_sent = repla_sent.split(' . ')\n",
    "                    repla_sent = random.choice(repla_sent)\n",
    "                    self.NSP_data.append(((cand_sentences[i], repla_sent), 0))\n",
    "\n",
    "\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        pair, label = self.NSP_data[idx]\n",
    "        return self.tokenizer(*pair, truncation=True, max_length=self.max_length), label\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.NSP_data)\n",
    "\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    tokens = [item[0] for item in batch]\n",
    "    \n",
    "    tokens = data_collator(tokens)  #output will be {\"input_ids\" : ? , \"token_type_ids\" : ?, \"attention_mask\" : ?, \"labels\" : ?}\n",
    "\n",
    "\n",
    "    mask = (tokens[\"labels\"] == -100)\n",
    "\n",
    "    mlm_pred_positions = tokens[\"labels\"].clone()\n",
    "    mlm_pred_positions[mask] = 0\n",
    "    mlm_pred_positions_batch, mlm_pred_positions_pos = torch.nonzero(mlm_pred_positions, as_tuple=True)\n",
    "    mlm_preds = torch.nonzero(mlm_pred_positions)\n",
    "    tokens[\"labels\"] = tokens[\"labels\"][mlm_preds[:,0], mlm_preds[:,1]]\n",
    "    \n",
    "\n",
    "    #swap values 1<->0 in attention_mask as nn.TransformerEncoderLayer uses a different convention\n",
    "    mask = (tokens[\"attention_mask\"]==1)\n",
    "    tokens[\"attention_mask\"][mask] = 0\n",
    "    tokens[\"attention_mask\"][~mask] = 1\n",
    "    tokens[\"attention_mask\"] = tokens[\"attention_mask\"].bool()\n",
    "            \n",
    "    nsp_labels = [item[1] for item in batch]\n",
    "    \n",
    "    #nsp_labels = torch.LongTensor(nsp_labels)\n",
    "    X = {k:v for k, v in tokens.items() if k != \"labels\"}  #X excludes \"labels\", as this concerns more the output.\n",
    "    X[\"labels_batch\"] = mlm_pred_positions_batch\n",
    "    X[\"labels_positions\"] = mlm_pred_positions_pos\n",
    "    #y = (tokens[\"labels\"], nsp_labels)\n",
    "    y = (tokens[\"labels\"], torch.zeros(len(batch), dtype=torch.int64))\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8092e090-b46a-4546-9499-fc1502bdbdb2",
   "metadata": {},
   "source": [
    "## Let's now build the BERT model and custom criterion used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "id": "f9ba705e-e013-4d19-a385-d2bb114208f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "#custom model\n",
    "class BertPretraining(nn.Module):\n",
    "    def __init__(self, vocab_size, num_hiddens, ffn_num_hiddens, num_heads, num_blks, dropout, max_len):\n",
    "        super().__init__()\n",
    "        self.num_hiddens=num_hiddens\n",
    "        self.max_len = max_len\n",
    "        self.token_embedding = nn.Embedding(vocab_size, num_hiddens)\n",
    "        self.segment_embedding = nn.Embedding(2, num_hiddens)\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1,max_len, num_hiddens))\n",
    "        self.blks = nn.Sequential()\n",
    "        for i in range(num_blks):\n",
    "            self.blks.add_module(f\"{i}\", nn.TransformerEncoderLayer(d_model=num_hiddens, \n",
    "                                                                   nhead=num_heads,\n",
    "                                                                   dim_feedforward=ffn_num_hiddens,\n",
    "                                                                   dropout=dropout, \n",
    "                                                                   batch_first=True))\n",
    "            \n",
    "        self.mlp = nn.Sequential(nn.LazyLinear(num_hiddens), nn.ReLU(), nn.LayerNorm(num_hiddens), nn.LazyLinear(vocab_size))\n",
    "\n",
    "        self.nsp = nn.Sequential(nn.LazyLinear(num_hiddens), nn.Tanh(), nn.LazyLinear(2))\n",
    "\n",
    "    \n",
    "            \n",
    "\n",
    "    def forward(self, input_ids, token_type_ids, attention_mask, labels_batch, labels_positions):\n",
    "        X = self.token_embedding(input_ids)*math.sqrt(self.num_hiddens) + self.segment_embedding(token_type_ids)\n",
    "        X = X + self.pos_embedding[:, :X.shape[1], :]\n",
    "        for blk in self.blks:\n",
    "            X = blk(X, src_key_padding_mask=attention_mask)\n",
    "\n",
    "        ##get mlm_Y_hat prediction\n",
    "        masked_X = X[labels_batch, labels_positions]\n",
    "        mlm_Y_hat = self.mlp(masked_X)\n",
    "\n",
    "        #get nsp_Y_hat prediction\n",
    "        nsp_Y_hat  =self.nsp(X[:,0,:])\n",
    "\n",
    "        return mlm_Y_hat, nsp_Y_hat\n",
    "\n",
    "\n",
    "#custom criterion\n",
    "class CriterionBert(torch.nn.modules.loss._Loss):\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        loss = nn.CrossEntropyLoss()\n",
    "        \n",
    "        #compute masked language model loss:\n",
    "        mlm_l = loss(y_pred[0], y_true[0].reshape(-1))\n",
    "\n",
    "        #compute next sentence prediction loss:\n",
    "        nsp_l = loss(y_pred[1], y_true[1].reshape(-1))\n",
    "\n",
    "        #total loss\n",
    "        l = mlm_l + nsp_l\n",
    "\n",
    "        return l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ce5b5a-c2d3-4c4a-88dc-c09d09603403",
   "metadata": {},
   "source": [
    "## Optuna based objective function for hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "id": "a5017b75-724d-4dee-827a-7942c6e71b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#approach using optuna\n",
    "\n",
    "import optuna\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "#define model, optimizer and criterion    \n",
    "model = BertPretraining(vocab_size = len(tokenizer), num_hiddens=128, ffn_num_hiddens=256, num_heads=2, num_blks=2, \n",
    "                        dropout=0.2, max_len=15)\n",
    "device = torch.device(\"cuda:0\")\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "\n",
    "    #define hyperparameter search space. Keep it simple and just search \"lr\" and \"weight_decay\"\n",
    "    \n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e0, log=True)\n",
    "    \n",
    "    weight_decay = trial.suggest_float(\"Ridge\", 1e-5, 1e0, log=True)\n",
    "\n",
    "    #define optimizer and criterion\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    \n",
    "    criterion = CriterionBert()\n",
    "\n",
    "\n",
    "    #define data and cross-validation splitting values\n",
    "\n",
    "    dataset = MyDataset(tokenizer, train_data, 15)\n",
    "\n",
    "    kf = KFold(n_splits=3, shuffle=True)\n",
    "    fold_accuracies=[]\n",
    "\n",
    "    for train_idx, val_idx in kf.split(range(len(dataset))):\n",
    "        train_subset = torch.utils.data.Subset(dataset, train_idx)\n",
    "        val_subset = torch.utils.data.Subset(dataset, val_idx)\n",
    "\n",
    "        train_dataloader = torch.utils.data.DataLoader(train_subset, batch_size=256, shuffle=True, pin_memory=True, num_workers=10, \n",
    "                                                       collate_fn = collate_fn, prefetch_factor=2, multiprocessing_context='fork')\n",
    "\n",
    "        val_dataloader = torch.utils.data.DataLoader(train_subset, batch_size=256, shuffle=False, pin_memory=True, num_workers=10, \n",
    "                                                    collate_fn = collate_fn, prefetch_factor=2, multiprocessing_context='fork')\n",
    "\n",
    "        num_epochs=5\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            for inputs, labels in train_dataloader:\n",
    "                #move data to GPU\n",
    "                inputs = {k:v.to(device, non_blocking=True) for k, v in inputs.items()}\n",
    "                labels_mlp = labels[0].to(device, non_blocking=True)\n",
    "                labels_nsp = labels[1].to(device, non_blocking=True)\n",
    "                labels = (labels_mlp, labels_nsp)\n",
    "                \n",
    "                outputs = model(**inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_dataloader:\n",
    "                inputs = {k:v.to(device, non_blocking=True) for k, v in inputs.items()}\n",
    "                labels_mlm = labels[0].to(device, non_blocking=True)\n",
    "                labels_nsp = labels[1].to(device, non_blocking=True)\n",
    "                total += labels_mlm.shape[0] + labels_nsp.shape[0]\n",
    "                outputs = model(**inputs) #mlm_Y_hat and nsp_Y_hat\n",
    "                #get predictions from mlm_Y_hat\n",
    "                _, mlm_preds = torch.max(outputs[0],1)\n",
    "                correct += (mlm_preds==labels_mlm).sum().item()\n",
    "\n",
    "                #get predictions from nsp_Y_hat\n",
    "                _, nsp_preds = torch.max(outputs[1],1)\n",
    "                correct += (nsp_preds==labels_nsp).sum().item()\n",
    "\n",
    "        fold_accuracies.append(correct/total)\n",
    "\n",
    "    return sum(fold_accuracies)/len(fold_accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b52e190-2f43-40eb-a56b-c5cd21904a72",
   "metadata": {},
   "source": [
    "## Run the optuna study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "id": "f3dd7898-9037-447f-802c-58d34f5047d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-30 10:18:34,467] A new study created in memory with name: no-name-ce7afe24-b26b-44dc-b5ca-651ab22ea2e2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be0a98f737b24d43b2b278b6d97c1d78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-08-30 10:18:38,046] Trial 2 failed with parameters: {'lr': 3.26946681009366e-05, 'Ridge': 0.0002053279746605567} because of the following error: RuntimeError('one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [128, 384]], which is output 0 of AsStridedBackward0, is at version 11; expected version 10 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).').\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ildar/anaconda3/envs/play/lib/python3.12/site-packages/optuna/study/_optimize.py\", line 201, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_17234/3522909497.py\", line 59, in objective\n",
      "    loss.backward()\n",
      "  File \"/home/ildar/anaconda3/envs/play/lib/python3.12/site-packages/torch/_tensor.py\", line 626, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"/home/ildar/anaconda3/envs/play/lib/python3.12/site-packages/torch/autograd/__init__.py\", line 347, in backward\n",
      "    _engine_run_backward(\n",
      "  File \"/home/ildar/anaconda3/envs/play/lib/python3.12/site-packages/torch/autograd/graph.py\", line 823, in _engine_run_backward\n",
      "    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [128, 384]], which is output 0 of AsStridedBackward0, is at version 11; expected version 10 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).\n",
      "[W 2025-08-30 10:18:38,049] Trial 2 failed with value None.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid 909083) exited unexpectedly with exit code 1. Details are lost due to multiprocessing. Rerunning with num_workers=0 may give better error trace.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[619], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m study\u001b[38;5;241m.\u001b[39moptimize(objective, n_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, show_progress_bar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/play/lib/python3.12/site-packages/optuna/study/study.py:490\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    390\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    397\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    398\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    399\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    400\u001b[0m \n\u001b[1;32m    401\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    489\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 490\u001b[0m     _optimize(\n\u001b[1;32m    491\u001b[0m         study\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    492\u001b[0m         func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[1;32m    493\u001b[0m         n_trials\u001b[38;5;241m=\u001b[39mn_trials,\n\u001b[1;32m    494\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m    495\u001b[0m         n_jobs\u001b[38;5;241m=\u001b[39mn_jobs,\n\u001b[1;32m    496\u001b[0m         catch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mtuple\u001b[39m(catch) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(catch, Iterable) \u001b[38;5;28;01melse\u001b[39;00m (catch,),\n\u001b[1;32m    497\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[1;32m    498\u001b[0m         gc_after_trial\u001b[38;5;241m=\u001b[39mgc_after_trial,\n\u001b[1;32m    499\u001b[0m         show_progress_bar\u001b[38;5;241m=\u001b[39mshow_progress_bar,\n\u001b[1;32m    500\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/play/lib/python3.12/site-packages/optuna/study/_optimize.py:82\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     79\u001b[0m time_start \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[1;32m     80\u001b[0m futures: \u001b[38;5;28mset\u001b[39m[Future] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[0;32m---> 82\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ThreadPoolExecutor(max_workers\u001b[38;5;241m=\u001b[39mn_jobs) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m n_submitted_trials \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mcount():\n\u001b[1;32m     84\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m study\u001b[38;5;241m.\u001b[39m_stop_flag:\n",
      "File \u001b[0;32m~/anaconda3/envs/play/lib/python3.12/concurrent/futures/_base.py:647\u001b[0m, in \u001b[0;36mExecutor.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    646\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, exc_type, exc_val, exc_tb):\n\u001b[0;32m--> 647\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshutdown(wait\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    648\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/play/lib/python3.12/concurrent/futures/thread.py:238\u001b[0m, in \u001b[0;36mThreadPoolExecutor.shutdown\u001b[0;34m(self, wait, cancel_futures)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads:\n\u001b[0;32m--> 238\u001b[0m         t\u001b[38;5;241m.\u001b[39mjoin()\n",
      "File \u001b[0;32m~/anaconda3/envs/play/lib/python3.12/threading.py:1147\u001b[0m, in \u001b[0;36mThread.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot join current thread\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1147\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_tstate_lock()\n\u001b[1;32m   1148\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1149\u001b[0m     \u001b[38;5;66;03m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[1;32m   1150\u001b[0m     \u001b[38;5;66;03m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[1;32m   1151\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_tstate_lock(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(timeout, \u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[0;32m~/anaconda3/envs/play/lib/python3.12/threading.py:1167\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   1166\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1167\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m lock\u001b[38;5;241m.\u001b[39macquire(block, timeout):\n\u001b[1;32m   1168\u001b[0m         lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m   1169\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stop()\n",
      "File \u001b[0;32m~/anaconda3/envs/play/lib/python3.12/site-packages/torch/utils/data/_utils/signal_handling.py:73\u001b[0m, in \u001b[0;36m_set_SIGCHLD_handler.<locals>.handler\u001b[0;34m(signum, frame)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhandler\u001b[39m(signum, frame):\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;66;03m# This following call uses `waitid` with WNOHANG from C side. Therefore,\u001b[39;00m\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;66;03m# Python can still get and update the process status successfully.\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     _error_if_any_worker_fails()\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m previous_handler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     75\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(previous_handler)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid 909083) exited unexpectedly with exit code 1. Details are lost due to multiprocessing. Rerunning with num_workers=0 may give better error trace."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-08-30 10:18:38,140] Trial 1 failed with parameters: {'lr': 0.004539507271152895, 'Ridge': 7.24419158627919e-05} because of the following error: RuntimeError('one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [128, 28996]], which is output 0 of AsStridedBackward0, is at version 19; expected version 18 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).').\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ildar/anaconda3/envs/play/lib/python3.12/site-packages/optuna/study/_optimize.py\", line 201, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_17234/3522909497.py\", line 59, in objective\n",
      "    loss.backward()\n",
      "  File \"/home/ildar/anaconda3/envs/play/lib/python3.12/site-packages/torch/_tensor.py\", line 626, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"/home/ildar/anaconda3/envs/play/lib/python3.12/site-packages/torch/autograd/__init__.py\", line 347, in backward\n",
      "    _engine_run_backward(\n",
      "  File \"/home/ildar/anaconda3/envs/play/lib/python3.12/site-packages/torch/autograd/graph.py\", line 823, in _engine_run_backward\n",
      "    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [128, 28996]], which is output 0 of AsStridedBackward0, is at version 19; expected version 18 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).\n",
      "[W 2025-08-30 10:18:38,141] Trial 1 failed with value None.\n",
      "[I 2025-08-30 10:19:31,403] Trial 0 finished with value: 0.3838864287104826 and parameters: {'lr': 0.03661448744453229, 'Ridge': 0.0003867779203817514}. Best is trial 0 with value: 0.3838864287104826.\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=3, n_jobs=10, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2e054d-4773-4702-9d99-e67503d28aaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a8a455ae-0fd7-4d4f-9eed-4c9aedae3aca",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0c352d-f293-4b6d-95e1-fe1a54bf2afd",
   "metadata": {},
   "source": [
    "### In this appendix we play around with the dataset to get a feel for its output signature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323ebaa7-b8bb-4941-8565-e7d39e9db468",
   "metadata": {},
   "source": [
    "#### The dataset uses the tokenizer to create the __getitem__. Let's see what it looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "id": "fa72d518-7bcd-4803-86f3-581ad61d3882",
   "metadata": {},
   "outputs": [],
   "source": [
    "L = [p[\"text\"] for p in train_data]\n",
    "\n",
    "import random\n",
    "NSP_data = []  #elements will be (sen1, sen2, True/False) depending on whether sen2 follows sen1\n",
    "\n",
    "for paragraph in L:\n",
    "    cand_sentences = paragraph.split(' . ')\n",
    "    number_of_sent = len(cand_sentences)\n",
    "    if number_of_sent<2:\n",
    "        continue\n",
    "    else:\n",
    "        for i in range(number_of_sent-1):\n",
    "            if random.random()<0.5:\n",
    "                NSP_data.append(((cand_sentences[i], cand_sentences[i+1]), 1))\n",
    "            #look at cand_sentences[i] and cand_sentences[i+1]\n",
    "            else:\n",
    "                repla_sent = random.choice(L)\n",
    "                repla_sent = repla_sent.split(' . ')\n",
    "                repla_sent = random.choice(repla_sent)\n",
    "                NSP_data.append(((cand_sentences[i], repla_sent), 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "id": "c45d272a-bc22-4bbe-ae19-f2d80097c3a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 1799, 1122, 5366, 1103, 2530, 1956, 1104, 1103, 1326, 117, 1122, 1145, 9315, 2967, 27939, 117, 1216, 1112, 1543, 1103, 1342, 1167, 1111, 5389, 3970, 1111, 1326, 25551, 1116, 102, 134, 134, 22130, 134, 134, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 585,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair, _ = NSP_data[5]\n",
    "tokenizer(*pair, truncation=True, max_length=70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232c7ade-e7ca-4b54-a9ed-78f93fbbe521",
   "metadata": {},
   "source": [
    "#### Let's see how collate function pads/batches data together "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "id": "3b49b125-e5dd-4296-b804-5fd3bd93dc64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,   103,  1122,  5366,  1103,  2530,  1956,  1104,  1103,  1326,\n",
       "           117,  1122,  1145,  9315,  2967, 27939,   117,  1216,  1112,  1543,\n",
       "          1103,  1342,  1167,  1111,   103,   103,   103,  1326, 25551,  1116,\n",
       "           102,   134,   134, 22130,   134,   103,   102,     0,     0,     0,\n",
       "             0,     0],\n",
       "        [  101, 23543,  5592,   103,  1777, 10942, 25028,  1105,  3996, 15375,\n",
       "           103, 17784, 18504, 12355,  1241,  1608,  1121,  2166, 10813,   117,\n",
       "          1373,  1114,   103,   103,  3464, 17758,  1563,  1900, 26713, 18763,\n",
       "         16075, 10946,   102,   138,   103,  1264,  1104,  5094,  8630,  1103,\n",
       "          5444,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[ -100,  1799,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  5389,  3970,  1111,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,   134,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100],\n",
       "        [ -100,  -100,  -100, 20089,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "         22437,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100, 12226,  3781,  -100,  -100,  -100,  -100,  -100,  3031,\n",
       "          -100,  -100,  -100,  -100,  1415,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100]])}"
      ]
     },
     "execution_count": 588,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair1, _ = NSP_data[5]\n",
    "pair2, _ = NSP_data[6]\n",
    "\n",
    "x = tokenizer(*pair1, truncation=True, max_length=70)\n",
    "y = tokenizer(*pair2, truncation=True, max_length=70)\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True)\n",
    "\n",
    "tokens = data_collator([x,y])\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "id": "aa5de5f2-a93b-4845-9cf5-d890a7a7c6c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: {'input_ids': tensor([[  101, 14895, 21006,  1185, 12226,  3781,  3464,   102, 12226,  3781,\n",
      "          3464,  1104,  1103,  2651,   102],\n",
      "        [  101, 18653,  1643, 26179,  1158,  1103,  1269, 11970,  1104, 12394,\n",
      "          1105,  1842,   137,   118,   102],\n",
      "        [  101,   103,  1342,  1310,  1718,   103,  1333,   102, 19729,  1122,\n",
      "          5366,  1103,   103,  1956,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]]), 'attention_mask': tensor([[False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False]]), 'labels_batch': tensor([2, 2, 2, 2]), 'labels_positions': tensor([ 1,  5,  8, 12])} \n",
      " y: (tensor([1109, 1107, 1799, 2530]), tensor([0, 0, 0]))\n"
     ]
    }
   ],
   "source": [
    "dataset = MyDataset(tokenizer, train_data, 15)\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=3, collate_fn=collate_fn)\n",
    "\n",
    "X, y = next(iter(dataloader))\n",
    "print(\"X:\", X,\"\\n\",\"y:\", y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
