{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8dce700-426a-4deb-a966-257cfc082d40",
   "metadata": {},
   "source": [
    "# This notebook will pretrain a BERT model on the WikiText-2 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42fec08-4fe2-4fd4-a212-a58a495c147a",
   "metadata": {},
   "source": [
    "## Let's first load the WikiText-2 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e12e0f1-42ea-4558-b2da-a9687dd15194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 36718\n",
      "Number of validation examples: 3760\n",
      "Number of test examples: 4358\n"
     ]
    }
   ],
   "source": [
    "#let's get the data..let's try to use the Hugging Face API\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the wikitext-2-raw-v1 configuration of the wikitext dataset\n",
    "raw_datasets = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "\n",
    "# The 'raw_datasets' object will now contain the dataset, typically split into\n",
    "# 'train', 'validation', and 'test' splits.\n",
    "# You can access them like this:\n",
    "train_data = raw_datasets[\"train\"]\n",
    "validation_data = raw_datasets[\"validation\"]\n",
    "test_data = raw_datasets[\"test\"]\n",
    "\n",
    "print(f\"Number of training examples: {len(train_data)}\")\n",
    "print(f\"Number of validation examples: {len(validation_data)}\")\n",
    "print(f\"Number of test examples: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a7bd45-2eec-4ce4-b1e7-eed125f55633",
   "metadata": {},
   "source": [
    "## Let's create a custom Dataset and Collate function for dynamic padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8edb960f-504a-4a86-ad59-15b8e9c95216",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-01 05:06:53.958370: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-09-01 05:06:53.966594: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1756696013.976669 1731760 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1756696013.979628 1731760 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-09-01 05:06:53.990078: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from transformers import AutoTokenizer, DataCollatorForLanguageModeling\n",
    "import torch\n",
    "\n",
    "checkpoint = \"google-bert/bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, tokenizer, data, max_length):\n",
    "        self.tokenizer=tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        L = [p[\"text\"] for p in data]\n",
    "        self.NSP_data = []\n",
    "\n",
    "        for paragraph in L:\n",
    "            cand_sentences = paragraph.split(' . ')\n",
    "            number_of_sentences = len(cand_sentences)\n",
    "            if number_of_sentences<2:\n",
    "                continue\n",
    "            else:\n",
    "                for i in range(number_of_sentences-1):\n",
    "                    if random.random()<0.5:\n",
    "                        self.NSP_data.append(((cand_sentences[i], cand_sentences[i+1]), 1))\n",
    "                else:\n",
    "                    repla_sent = random.choice(L)\n",
    "                    repla_sent = repla_sent.split(' . ')\n",
    "                    repla_sent = random.choice(repla_sent)\n",
    "                    self.NSP_data.append(((cand_sentences[i], repla_sent), 0))\n",
    "\n",
    "\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        pair, label = self.NSP_data[idx]\n",
    "        return self.tokenizer(*pair, truncation=True, max_length=self.max_length), label\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.NSP_data)\n",
    "\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    tokens = [item[0] for item in batch]\n",
    "    \n",
    "    tokens = data_collator(tokens)  #output will be {\"input_ids\" : ? , \"token_type_ids\" : ?, \"attention_mask\" : ?, \"labels\" : ?}\n",
    "\n",
    "\n",
    "    mask = (tokens[\"labels\"] == -100)\n",
    "\n",
    "    mlm_pred_positions = tokens[\"labels\"].clone()\n",
    "    mlm_pred_positions[mask] = 0\n",
    "    mlm_pred_positions_batch, mlm_pred_positions_pos = torch.nonzero(mlm_pred_positions, as_tuple=True)\n",
    "    mlm_preds = torch.nonzero(mlm_pred_positions)\n",
    "    tokens[\"labels\"] = tokens[\"labels\"][mlm_preds[:,0], mlm_preds[:,1]]\n",
    "    \n",
    "\n",
    "    #swap values 1<->0 in attention_mask as nn.TransformerEncoderLayer uses a different convention\n",
    "    mask = (tokens[\"attention_mask\"]==1)\n",
    "    tokens[\"attention_mask\"][mask] = 0\n",
    "    tokens[\"attention_mask\"][~mask] = 1\n",
    "    tokens[\"attention_mask\"] = tokens[\"attention_mask\"].bool()\n",
    "            \n",
    "    nsp_labels = [item[1] for item in batch]\n",
    "    \n",
    "    nsp_labels = torch.LongTensor(nsp_labels)\n",
    "    X = {k:v for k, v in tokens.items() if k != \"labels\"}  #X excludes \"labels\", as this concerns more the output.\n",
    "    X[\"labels_batch\"] = mlm_pred_positions_batch\n",
    "    X[\"labels_positions\"] = mlm_pred_positions_pos\n",
    "    y = (tokens[\"labels\"], nsp_labels)\n",
    "    #y = (tokens[\"labels\"], torch.zeros(len(batch), dtype=torch.int64))\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8092e090-b46a-4546-9499-fc1502bdbdb2",
   "metadata": {},
   "source": [
    "## Let's now build the BERT model and custom criterion used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9ba705e-e013-4d19-a385-d2bb114208f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "#custom model\n",
    "class BertPretraining(nn.Module):\n",
    "    def __init__(self, vocab_size, num_hiddens, ffn_num_hiddens, num_heads, num_blks, dropout, max_len):\n",
    "        super().__init__()\n",
    "        self.num_hiddens=num_hiddens\n",
    "        self.max_len = max_len\n",
    "        self.token_embedding = nn.Embedding(vocab_size, num_hiddens)\n",
    "        self.segment_embedding = nn.Embedding(2, num_hiddens)\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1,max_len, num_hiddens))\n",
    "        self.blks = nn.Sequential()\n",
    "        for i in range(num_blks):\n",
    "            self.blks.add_module(f\"{i}\", nn.TransformerEncoderLayer(d_model=num_hiddens, \n",
    "                                                                   nhead=num_heads,\n",
    "                                                                   dim_feedforward=ffn_num_hiddens,\n",
    "                                                                   dropout=dropout, \n",
    "                                                                   batch_first=True))\n",
    "            \n",
    "        self.mlp = nn.Sequential(nn.LazyLinear(num_hiddens), nn.ReLU(), nn.LayerNorm(num_hiddens), nn.LazyLinear(vocab_size))\n",
    "\n",
    "        self.nsp = nn.Sequential(nn.LazyLinear(num_hiddens), nn.Tanh(), nn.LazyLinear(2))\n",
    "\n",
    "    \n",
    "            \n",
    "\n",
    "    def forward(self, input_ids, token_type_ids, attention_mask, labels_batch, labels_positions):\n",
    "        X = self.token_embedding(input_ids)*math.sqrt(self.num_hiddens) + self.segment_embedding(token_type_ids)\n",
    "        X = X + self.pos_embedding[:, :X.shape[1], :]\n",
    "        for blk in self.blks:\n",
    "            X = blk(X, src_key_padding_mask=attention_mask)\n",
    "\n",
    "        ##get mlm_Y_hat prediction\n",
    "        masked_X = X[labels_batch, labels_positions]\n",
    "        mlm_Y_hat = self.mlp(masked_X)\n",
    "\n",
    "        #get nsp_Y_hat prediction\n",
    "        nsp_Y_hat  =self.nsp(X[:,0,:])\n",
    "\n",
    "        return mlm_Y_hat, nsp_Y_hat\n",
    "\n",
    "    \n",
    "    def apply_init(self, inputs, init=None):\n",
    "        self.forward(**inputs)\n",
    "        if init is not None:\n",
    "            self.mlp.apply(init)\n",
    "            self.nsp.apply(init)\n",
    "            self.blks.apply(init)\n",
    "\n",
    "    def init_weights_xavier_uniform(self, m):\n",
    "        if hasattr(m, 'weight') and m.weight.dim()>=2:\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "\n",
    "#custom criterion\n",
    "class CriterionBert(torch.nn.modules.loss._Loss):\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        loss = nn.CrossEntropyLoss()\n",
    "        \n",
    "        #compute masked language model loss:\n",
    "        mlm_l = loss(y_pred[0], y_true[0].reshape(-1))\n",
    "\n",
    "        #compute next sentence prediction loss:\n",
    "        nsp_l = loss(y_pred[1], y_true[1].reshape(-1))\n",
    "\n",
    "        #total loss\n",
    "        l = mlm_l + nsp_l\n",
    "\n",
    "        return l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ce5b5a-c2d3-4c4a-88dc-c09d09603403",
   "metadata": {},
   "source": [
    "## Optuna based objective function for hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5017b75-724d-4dee-827a-7942c6e71b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#approach using optuna\n",
    "\n",
    "import optuna\n",
    "from sklearn.model_selection import KFold\n",
    "import gc\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "\n",
    "    #define hyperparameter search space. Keep it simple and just search \"lr\" and \"weight_decay\"\n",
    "    \n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e0, log=True)\n",
    "    \n",
    "    weight_decay = trial.suggest_float(\"Ridge\", 1e-5, 1e0, log=True)\n",
    "\n",
    "    #define model, optimizer and criterion    \n",
    "    model = BertPretraining(vocab_size = len(tokenizer), num_hiddens=128, ffn_num_hiddens=256, num_heads=2, num_blks=2, \n",
    "                            dropout=0.2, max_len=15)\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    model.to(device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    \n",
    "    criterion = CriterionBert()\n",
    "\n",
    "\n",
    "    #define data and cross-validation splitting values\n",
    "\n",
    "    dataset = MyDataset(tokenizer, train_data, 15)\n",
    "\n",
    "    kf = KFold(n_splits=3, shuffle=True)\n",
    "    fold_accuracies=[]\n",
    "\n",
    "    for train_idx, val_idx in kf.split(range(len(dataset))):\n",
    "        \n",
    "        train_subset = torch.utils.data.Subset(dataset, train_idx)\n",
    "        val_subset = torch.utils.data.Subset(dataset, val_idx)\n",
    "\n",
    "        train_dataloader = torch.utils.data.DataLoader(train_subset, batch_size=1024, shuffle=True, pin_memory=True, num_workers=10, \n",
    "                                                       collate_fn = collate_fn, prefetch_factor=2, multiprocessing_context='fork')\n",
    "\n",
    "        val_dataloader = torch.utils.data.DataLoader(val_subset, batch_size=1024, shuffle=False, pin_memory=True, num_workers=10, \n",
    "                                                    collate_fn = collate_fn, prefetch_factor=2, multiprocessing_context='fork')\n",
    "        \n",
    "        #re-initialize weights here\n",
    "        init_inputs = (next(iter(train_dataloader)))[0]\n",
    "        for k, v in init_inputs.items():\n",
    "            v = v.to(device, non_blocking=True)\n",
    "            init_inputs[k]=v\n",
    "            \n",
    "        model.apply_init(init_inputs, model.init_weights_xavier_uniform)\n",
    "\n",
    "        num_epochs=5\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            for inputs, labels in train_dataloader:\n",
    "                #move data to GPU\n",
    "                for k,v in inputs.items():\n",
    "                    v = v.to(device, non_blocking=True)\n",
    "                    inputs[k] = v\n",
    "\n",
    "                labels_mlm = labels[0].to(device, non_blocking=True)\n",
    "                labels_nsp = labels[1].to(device, non_blocking=True)\n",
    "                labels = (labels_mlm, labels_nsp)\n",
    "                \n",
    "                outputs = model(**inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                del inputs\n",
    "                del labels\n",
    "                del loss\n",
    "                del outputs\n",
    "\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_dataloader:\n",
    "\n",
    "                for k, v in inputs.items():\n",
    "                    v = v.to(device, non_blocking=True)\n",
    "                    inputs[k] = v\n",
    "\n",
    "                labels_mlm = labels[0].to(device, non_blocking=True)\n",
    "                labels_nsp = labels[1].to(device, non_blocking=True)\n",
    "                labels = (labels_mlm, labels_nsp)\n",
    "                \n",
    "                total += labels[0].shape[0] + labels[1].shape[0]\n",
    "                outputs = model(**inputs) #mlm_Y_hat and nsp_Y_hat\n",
    "                #get predictions from mlm_Y_hat\n",
    "                _, mlm_preds = torch.max(outputs[0],1)\n",
    "                correct += (mlm_preds==labels[0]).sum().item()\n",
    "\n",
    "                #get predictions from nsp_Y_hat\n",
    "                _, nsp_preds = torch.max(outputs[1],1)\n",
    "                correct += (nsp_preds==labels[1]).sum().item()\n",
    "\n",
    "                del inputs\n",
    "                del labels\n",
    "                del outputs\n",
    "                del mlm_preds\n",
    "                del nsp_preds\n",
    "\n",
    "        fold_accuracies.append(correct/total)\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    return sum(fold_accuracies)/len(fold_accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b52e190-2f43-40eb-a56b-c5cd21904a72",
   "metadata": {},
   "source": [
    "## Run the optuna study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3dd7898-9037-447f-802c-58d34f5047d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-01 05:07:06,205] A new study created in memory with name: no-name-50396b93-ec55-45dd-b0d6-8286f8c6c045\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f32773923f8c45818e2ab7fb9a51a178",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-01 05:09:32,307] Trial 0 finished with value: 0.30118768338327717 and parameters: {'lr': 0.0010611174682805564, 'Ridge': 0.09270599575803558}. Best is trial 0 with value: 0.30118768338327717.\n",
      "[I 2025-09-01 05:09:32,325] Trial 2 finished with value: 0.2810787355447523 and parameters: {'lr': 0.005973141263366834, 'Ridge': 0.00019930307397072425}. Best is trial 0 with value: 0.30118768338327717.\n",
      "[I 2025-09-01 05:09:35,154] Trial 1 finished with value: 0.27845738822513183 and parameters: {'lr': 0.046420052131932385, 'Ridge': 0.02903435311088126}. Best is trial 0 with value: 0.30118768338327717.\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=3, n_jobs=3, show_progress_bar=True)\n",
    "torch.cuda.empty_cache()  #clear reserved memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a455ae-0fd7-4d4f-9eed-4c9aedae3aca",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0c352d-f293-4b6d-95e1-fe1a54bf2afd",
   "metadata": {},
   "source": [
    "### In this appendix we play around with the dataset to get a feel for its output signature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323ebaa7-b8bb-4941-8565-e7d39e9db468",
   "metadata": {},
   "source": [
    "#### The dataset uses the tokenizer to create the __getitem__. Let's see what it looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "id": "fa72d518-7bcd-4803-86f3-581ad61d3882",
   "metadata": {},
   "outputs": [],
   "source": [
    "L = [p[\"text\"] for p in train_data]\n",
    "\n",
    "import random\n",
    "NSP_data = []  #elements will be (sen1, sen2, True/False) depending on whether sen2 follows sen1\n",
    "\n",
    "for paragraph in L:\n",
    "    cand_sentences = paragraph.split(' . ')\n",
    "    number_of_sent = len(cand_sentences)\n",
    "    if number_of_sent<2:\n",
    "        continue\n",
    "    else:\n",
    "        for i in range(number_of_sent-1):\n",
    "            if random.random()<0.5:\n",
    "                NSP_data.append(((cand_sentences[i], cand_sentences[i+1]), 1))\n",
    "            #look at cand_sentences[i] and cand_sentences[i+1]\n",
    "            else:\n",
    "                repla_sent = random.choice(L)\n",
    "                repla_sent = repla_sent.split(' . ')\n",
    "                repla_sent = random.choice(repla_sent)\n",
    "                NSP_data.append(((cand_sentences[i], repla_sent), 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "id": "c45d272a-bc22-4bbe-ae19-f2d80097c3a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 1799, 1122, 5366, 1103, 2530, 1956, 1104, 1103, 1326, 117, 1122, 1145, 9315, 2967, 27939, 117, 1216, 1112, 1543, 1103, 1342, 1167, 1111, 5389, 3970, 1111, 1326, 25551, 1116, 102, 134, 134, 22130, 134, 134, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 585,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair, _ = NSP_data[5]\n",
    "tokenizer(*pair, truncation=True, max_length=70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232c7ade-e7ca-4b54-a9ed-78f93fbbe521",
   "metadata": {},
   "source": [
    "#### Let's see how collate function pads/batches data together "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "id": "3b49b125-e5dd-4296-b804-5fd3bd93dc64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,   103,  1122,  5366,  1103,  2530,  1956,  1104,  1103,  1326,\n",
       "           117,  1122,  1145,  9315,  2967, 27939,   117,  1216,  1112,  1543,\n",
       "          1103,  1342,  1167,  1111,   103,   103,   103,  1326, 25551,  1116,\n",
       "           102,   134,   134, 22130,   134,   103,   102,     0,     0,     0,\n",
       "             0,     0],\n",
       "        [  101, 23543,  5592,   103,  1777, 10942, 25028,  1105,  3996, 15375,\n",
       "           103, 17784, 18504, 12355,  1241,  1608,  1121,  2166, 10813,   117,\n",
       "          1373,  1114,   103,   103,  3464, 17758,  1563,  1900, 26713, 18763,\n",
       "         16075, 10946,   102,   138,   103,  1264,  1104,  5094,  8630,  1103,\n",
       "          5444,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[ -100,  1799,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  5389,  3970,  1111,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,   134,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100],\n",
       "        [ -100,  -100,  -100, 20089,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "         22437,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100, 12226,  3781,  -100,  -100,  -100,  -100,  -100,  3031,\n",
       "          -100,  -100,  -100,  -100,  1415,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100]])}"
      ]
     },
     "execution_count": 588,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair1, _ = NSP_data[5]\n",
    "pair2, _ = NSP_data[6]\n",
    "\n",
    "x = tokenizer(*pair1, truncation=True, max_length=70)\n",
    "y = tokenizer(*pair2, truncation=True, max_length=70)\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True)\n",
    "\n",
    "tokens = data_collator([x,y])\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa5de5f2-a93b-4845-9cf5-d890a7a7c6c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: {'input_ids': tensor([[  101, 11930,   103,  1356,  1349,   103,  1999,   102, 18653,  1643,\n",
      "           103,  1158,   103,  1269,   102],\n",
      "        [  101, 18653,  1643, 26179,   103,  1103,  1269, 11970,  1104, 12394,\n",
      "          1105,   103,   137,   102,   102],\n",
      "        [  101, 18653,  1643, 26179,   103,  1103,  1269,   102,   134,  3506,\n",
      "          7085, 19975,  1105, 19415,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]]), 'attention_mask': tensor([[False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False]]), 'labels_batch': tensor([0, 0, 0, 0, 1, 1, 2]), 'labels_positions': tensor([ 2,  5, 10, 12,  4, 11,  4])} \n",
      " y: (tensor([ 1107,  1107, 26179,  1103,  1158,  1842,  1158]), tensor([1, 1, 0]))\n"
     ]
    }
   ],
   "source": [
    "dataset = MyDataset(tokenizer, train_data, 15)\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=3, collate_fn=collate_fn)\n",
    "\n",
    "X, y = next(iter(dataloader))\n",
    "print(\"X:\", X,\"\\n\",\"y:\", y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
