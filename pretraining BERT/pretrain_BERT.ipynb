{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8dce700-426a-4deb-a966-257cfc082d40",
   "metadata": {},
   "source": [
    "# This notebook will pretrain a BERT model on the WikiText-2 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "id": "023cbba3-0025-4160-a968-5001afbe18f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 36718\n",
      "Number of validation examples: 3760\n",
      "Number of test examples: 4358\n"
     ]
    }
   ],
   "source": [
    "#let's get the data..let's try to use the Hugging Face API\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the wikitext-2-raw-v1 configuration of the wikitext dataset\n",
    "raw_datasets = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "\n",
    "# The 'raw_datasets' object will now contain the dataset, typically split into\n",
    "# 'train', 'validation', and 'test' splits.\n",
    "# You can access them like this:\n",
    "train_data = raw_datasets[\"train\"]\n",
    "validation_data = raw_datasets[\"validation\"]\n",
    "test_data = raw_datasets[\"test\"]\n",
    "\n",
    "print(f\"Number of training examples: {len(train_data)}\")\n",
    "print(f\"Number of validation examples: {len(validation_data)}\")\n",
    "print(f\"Number of test examples: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "id": "8edb960f-504a-4a86-ad59-15b8e9c95216",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from transformers import AutoTokenizer, DataCollatorForLanguageModeling\n",
    "import torch\n",
    "\n",
    "checkpoint = \"google-bert/bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, tokenizer, data, max_length):\n",
    "        self.tokenizer=tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        L = [p[\"text\"] for p in data]\n",
    "        self.NSP_data = []\n",
    "\n",
    "        for paragraph in L:\n",
    "            cand_sentences = paragraph.split(' . ')\n",
    "            number_of_sentences = len(cand_sentences)\n",
    "            if number_of_sentences<2:\n",
    "                continue\n",
    "            else:\n",
    "                for i in range(number_of_sentences-1):\n",
    "                    if random.random()<0.5:\n",
    "                        self.NSP_data.append(((cand_sentences[i], cand_sentences[i+1]), True))\n",
    "                else:\n",
    "                    repla_sent = random.choice(L)\n",
    "                    repla_sent = repla_sent.split(' . ')\n",
    "                    repla_sent = random.choice(repla_sent)\n",
    "                    self.NSP_data.append(((cand_sentences[i], repla_sent), False))\n",
    "\n",
    "\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        pair, label = self.NSP_data[idx]\n",
    "        return self.tokenizer(*pair, truncation=True, max_length=self.max_length), label\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.NSP_data)\n",
    "\n",
    "dataset = MyDataset(tokenizer, train_data, 15)\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    tokens = [item[0] for item in batch]\n",
    "    tokens = data_collator(tokens)  #output will be {\"input_ids\" : ? , \"token_type_ids\" : ?, \"attention_mask\" : ?, \"labels\" : ?}\n",
    "\n",
    "    mlm_pred_positions = tokens[\"labels\"].clone()\n",
    "\n",
    "    mask = (tokens[\"labels\"] != -100)\n",
    "    indices = torch.arange(tokens[\"labels\"].shape[1])\n",
    "    for i in range(tokens[\"labels\"].shape[0]):\n",
    "        mlm_pred_positions[i][mask[i]] = indices[mask[i]]\n",
    "        mlm_pred_positions[i][~mask[i]] = 0\n",
    "        tokens[\"labels\"][i][~mask[i]] = 0\n",
    "    mlm_pred_positions = torch.flatten(mlm_pred_positions)\n",
    "            \n",
    "    nsp_labels = [item[1] for item in batch]\n",
    "    all_labels = tokens[\"labels\"]\n",
    "    X = {k:v for k, v in tokens.items() if k != \"labels\"}  #X excludes \"labels\", as this concerns more the output.\n",
    "    X[\"labels\"] = mlm_pred_positions\n",
    "    y = (tokens[\"labels\"], nsp_labels)\n",
    "    return X,y\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "id": "8e77ce8b-1292-4a89-8831-4a52f1837e64",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after function definition on line 10 (3304479224.py, line 13)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[449], line 13\u001b[0;36m\u001b[0m\n\u001b[0;31m    class CustomLoss(nn.Module):\u001b[0m\n\u001b[0m                                ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block after function definition on line 10\n"
     ]
    }
   ],
   "source": [
    "#let's build the model now\n",
    "from skorch import NeuralNetClassifier\n",
    "import torch.nn as nn\n",
    "\n",
    "class BertPretraining(nn.Module):\n",
    "    def __init__(self, vocab_size, num_hiddens, ffn_num_hiddens, num_heads, num_blks, dropout, max_len):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, num_hiddens)\n",
    "        self.segment_embedding = nn.Embedding(2, num_hiddens)\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1,max_len, num_hiddens))\n",
    "        self.blks = nn.Sequential()\n",
    "        for i in range(num_blks):\n",
    "            self.blks.add_module(f\"{i}\", nn.TransformerEncoderLayer(d_model=num_hiddens, \n",
    "                                                                   nhead=num_heads,\n",
    "                                                                   dim_feedforward=ffn_num_hiddens,\n",
    "                                                                   dropout=dropout, \n",
    "                                                                   batch_first=True))\n",
    "            \n",
    "\n",
    "    def __forward__(self):\n",
    "\n",
    "\n",
    "class CustomLoss(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "id": "125c9254-5307-4a66-9d0c-805b67ed6665",
   "metadata": {},
   "outputs": [],
   "source": [
    "forward = nn.Embedding(4,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "id": "368b91d1-5727-4c0a-a30d-6b80f6b92271",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7540, -2.0427,  0.7257],\n",
       "        [ 0.7540, -2.0427,  0.7257],\n",
       "        [ 0.7540, -2.0427,  0.7257],\n",
       "        [ 0.3545, -1.3122, -0.5113]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 507,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forward(torch.tensor([1,1,1,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "id": "05c5e301-5b3e-4799-9326-067c970d6674",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.3898, -1.0870,  0.4674], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 453,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forward(torch.tensor(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "4f8397c5-d14f-4132-93a6-ab69b2093554",
   "metadata": {},
   "outputs": [],
   "source": [
    "pair, label = NSP_data[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "1aaf5e3e-4fe9-45f3-99f2-3e5d80d89ec3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('While it retained the standard features of the series , it also underwent multiple adjustments , such as making the game more forgiving for series newcomers',\n",
       " 'Character designer Raita Honjou and composer Hitoshi Sakimoto both returned from previous entries , along with Valkyria Chronicles II director Takeshi Ozawa')"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "07242bdb-230d-4866-94a8-a645742fc7e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 1799, 1122, 5366, 1103, 2530, 1956, 1104, 1103, 1326, 117, 1122, 1145, 9315, 2967, 27939, 117, 1216, 1112, 1543, 1103, 1342, 1167, 1111, 5389, 3970, 1111, 1326, 25551, 1116, 102, 23543, 5592, 20089, 1777, 10942, 25028, 1105, 3996, 15375, 22437, 17784, 18504, 12355, 1241, 1608, 1121, 2166, 10813, 117, 1373, 1114, 12226, 3781, 3464, 17758, 1563, 1900, 26713, 3031, 16075, 10946, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 391,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tokenizer(*pair, truncation=True, max_length=70)\n",
    "x  #input_ids have length 63"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "117aa212-af7c-4c93-89d8-9e80b793563b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pair2, label2 = NSP_data[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "7a1aecec-7375-4baf-9b7f-154b33aa5b3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 23543, 5592, 20089, 1777, 10942, 25028, 1105, 3996, 15375, 22437, 17784, 18504, 12355, 1241, 1608, 1121, 2166, 10813, 117, 1373, 1114, 12226, 3781, 3464, 17758, 1563, 1900, 26713, 3031, 16075, 10946, 102, 134, 134, 134, 21452, 1158, 134, 134, 134, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = tokenizer(*pair2, truncation=True, max_length=70)\n",
    "y  #input_ids have length 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "74d6f71f-7177-4b8c-a6bc-1fbe6ec7847b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "ea8e453d-4105-4cb8-943e-db5ae2482e83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(tokens[\"input_ids\"].shape[1]*0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "9ff0cd4b-cdf4-4296-9a82-8228eea024b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  1799,  1122,  5366,  1103,  2530,  1956,  1104,  1103,  1326,\n",
       "           117,   103,  1145,  9315,  2967, 27939,   117,  1216,  1112,  1543,\n",
       "          1103,   103,  1167,  1111,   103,  3970,  1111,  1326, 19623,  1116,\n",
       "           102, 23543,  5592, 20089,  1777, 10942, 25028,  1105,  3996, 15375,\n",
       "         22437,   103, 18504, 12355,   103,  1608,  1121,  2166, 10813,   117,\n",
       "          1373,  1114, 12226,  3781,  3464, 17758,  1563,  1900, 26713,  3031,\n",
       "         16075, 10946,   102],\n",
       "        [  101, 23543,  5592, 20089,  1777, 10942, 25028,  1105,  3996, 15375,\n",
       "         22437, 17784, 18504,   103,  1241,  1608,   103,   103, 10813,   117,\n",
       "          1373,  1114, 12226,  3781,  3464,   103,  1563,   103, 26713,  3031,\n",
       "         16075, 10946,   102,   134,   134,   134,   103,  1158,   134,   134,\n",
       "           103,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'labels': tensor([[ -100,  -100,  -100,  5366,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  1122,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  1342,  -100,  -100,  5389,  -100,  -100,  -100, 25551,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100, 17784,  -100,  -100,  1241,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100],\n",
       "        [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100, 12355,  -100,  -100,  1121,  2166,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100, 17758,  -100,  1900,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100, 21452,  -100,  -100,  -100,\n",
       "           134,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100]])}"
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = data_collator([x,y])\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "5f844dd2-eef1-4596-8a2a-e434f49fb378",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -100,  -100,  -100,  5366,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  1122,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  1342,  -100,  -100,  5389,  -100,  -100,  -100, 25551,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100, 17784,  -100,  -100,  1241,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100],\n",
       "        [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100, 12355,  -100,  -100,  1121,  2166,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100, 17758,  -100,  1900,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100, 21452,  -100,  -100,  -100,\n",
       "           134,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100]])"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = tokens[\"labels\"]\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "464256d6-e701-4a84-99dc-195e5ff423c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "a9092b1d-98bf-44fc-b01a-88644de14789",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,     0,     2,     0,     0,     5,     0,     0,     0,     0,\n",
       "            10,     0,     0,     0,     0,     0,     0,    17,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,    42,     0,     0,    45,    46,     0,    48,     0,\n",
       "             0,     0,    52,     0,     0,     0,     0,     0,    58,     0,\n",
       "             0,     0,     0],\n",
       "        [ -100,  -100,  -100,  -100,  -100, 10942,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  1241,  -100,  1121,  2166,  -100,  -100,\n",
       "          1373,  -100,  -100,  -100,  -100,  -100,  -100,  1900,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,   134,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100]])"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = (z!=-100)\n",
    "z_copy = z.clone().detach()\n",
    "z_copy[0][mask[0]] = indices[mask[0]]\n",
    "z_copy[0][~mask[0]]= 0\n",
    "z_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "04dc8416-f8c0-4d69-b462-bacd13571913",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    0,     0,     2,     0,     0,     5,     0,     0,     0,     0,\n",
       "           10,     0,     0,     0,     0,     0,     0,    17,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,    42,     0,     0,    45,    46,     0,    48,     0,\n",
       "            0,     0,    52,     0,     0,     0,     0,     0,    58,     0,\n",
       "            0,     0,     0,  -100,  -100,  -100,  -100,  -100, 10942,  -100,\n",
       "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  1241,  -100,  1121,\n",
       "         2166,  -100,  -100,  1373,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "         1900,  -100,  -100,  -100,  -100,  -100,  -100,   134,  -100,  -100,\n",
       "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "         -100,  -100,  -100,  -100,  -100,  -100])"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_copy = torch.flatten(z_copy)\n",
    "z_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "id": "02f660f5-7e2d-456b-b1e5-619196dde8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "L = [p[\"text\"] for p in train_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "id": "c3fa4298-16ff-4d75-903b-7ca2c3ceffd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#at this point L is a list, where each element is paragraph or a collection of consecutive sentences\n",
    "#let's try to create a list with elements (sen1, sen2, True/False)\n",
    "#\n",
    "import random\n",
    "NSP_data = []  #elements will be (sen1, sen2, True/False) depending on whether sen2 follows sen1\n",
    "\n",
    "for paragraph in L:\n",
    "    cand_sentences = paragraph.split(' . ')\n",
    "    number_of_sent = len(cand_sentences)\n",
    "    if number_of_sent<2:\n",
    "        continue\n",
    "    else:\n",
    "        for i in range(number_of_sent-1):\n",
    "            if random.random()<0.5:\n",
    "                NSP_data.append(((cand_sentences[i], cand_sentences[i+1]), True))\n",
    "            #look at cand_sentences[i] and cand_sentences[i+1]\n",
    "            else:\n",
    "                repla_sent = random.choice(L)\n",
    "                repla_sent = repla_sent.split(' . ')\n",
    "                repla_sent = random.choice(repla_sent)\n",
    "                NSP_data.append(((cand_sentences[i], repla_sent), False))\n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "id": "65693951-4cf7-4c72-8dba-69a4cdc404cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73380"
      ]
     },
     "execution_count": 494,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(NSP_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "e6b1eb2d-a831-42bf-ad70-a07dde16d79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "checkpoint = \"google-bert/bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "40de40ed-b294-4305-bf26-a2e75508cbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pair, label = NSP_data[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "44cb168b-4894-4b3c-a422-4778caf5069c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(' The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II',\n",
       " 'While it retained the standard features of the series , it also underwent multiple adjustments , such as making the game more forgiving for series newcomers')"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "afbaa2e5-85c2-4e47-8c5c-c5a7d187e6db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "828dd4de-d492-4331-94a9-629b9799d1ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 1109, 1342, 1310, 1718, 1107, 1333, 102, 1799, 1122, 5366, 1103, 2530, 1956, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tokenizer(*pair, truncation=True, max_length=15)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "840f95a1-233d-44d9-aa84-421d8640a623",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 1109, 1342, 1310, 1718,  103, 1333,  102, 1799, 1122, 5366,  103,\n",
       "         2530, 1956,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[-100, -100, -100, -100, -100, 1107, -100, -100, -100, -100, -100, 1103,\n",
       "         -100, -100, -100]])}"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_collator([x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "468f89f4-5fb7-44d2-85f1-f1d357a308bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[PAD] :'"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([0,131])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "69310ff7-e971-4bee-bc86-dc2913cd41a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3., 4.], dtype=torch.float64, requires_grad=True)"
      ]
     },
     "execution_count": 430,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1,2,3,4], dtype=torch.float64, requires_grad=True)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "id": "fed97629-2877-4a0a-b078-e618d244a5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.dot(x,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "id": "3ed5f40f-9101-4edc-9c50-0da17eff4eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "id": "ffb635e2-2276-4d2e-abc2-ff1bb843fc6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 4., 6., 8.], dtype=torch.float64)"
      ]
     },
     "execution_count": 434,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "id": "797ecf70-c6e3-4ffa-a0dd-fd424b01e61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([2,3,4], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "id": "e59818dc-d3c5-4155-b933-03b07cc822b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4.,  9., 16.])"
      ]
     },
     "execution_count": 436,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "id": "ba36d3c4-624c-4228-b56b-aff968647b67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4., 6., 8.])"
      ]
     },
     "execution_count": 437,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x+x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "id": "ab157610-ecc4-47b5-8d94-e834b1ab1c14",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Only Tensors of floating point and complex dtype can require gradients",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[448], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m2\u001b[39m, requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Only Tensors of floating point and complex dtype can require gradients"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(2, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "id": "c5c8218d-370f-4140-8cb4-0fdfdaa444c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x**2 + 3*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "id": "8a5850d3-9d29-468c-86e2-6020a21f23fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "id": "9e1fbe7a-77a9-4cde-9603-0af85de6015a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.)"
      ]
     },
     "execution_count": 447,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "id": "6e52572e-8885-48fd-8d65-20a464b4717c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "print(f\"{i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "id": "daa8a170-95a9-49fb-bbc7-60b1c062aa57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 480,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L = [1,2,3,4]\n",
    "\n",
    "next(iter(L))\n",
    "next(iter(L))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "id": "bd5f4e1f-46a0-41aa-8ba4-1d5534ad6dee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi Ho are you', 'jwjfei']"
      ]
     },
     "execution_count": 491,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L = \"Hi Ho are you . jwjfei\"\n",
    "L.split(' . ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645b1f34-14fd-4587-8b77-5060e565cbab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
